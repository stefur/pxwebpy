---
title: Examples
jupyter: python3
---

```{python}
# | echo: false
from IPython import get_ipython
from rich.pretty import pretty_repr


# Define some nicer formatting for code output
def plain_pretty_formatter(value):
    return pretty_repr(value, max_depth=3, max_length=5, max_string=50, expand_all=True)


ip = get_ipython()
ip.display_formatter.formatters["text/plain"] = plain_pretty_formatter

```

::: {.callout-note}
This library is not affiliated with the PxWeb project. These examples do not go into detail about how the PxWeb API behaves or responds. For more information about the API itself, check out the official [specification](https://github.com/PxTools/PxApiSpecs).
:::

## Basic setup and exploration
The first step is to set up a PxDatabase object to use.

```{python}
from pxwebpy import PxDatabase

# Use the builtin known database instead of a URL
db = PxDatabase("scb")

db
```

We can check out information about the API, including languages supported, by using `~~.PxDatabase.get_config()`.

```{python}
db.get_config()
```

If we want to change the language, we can do so by changing an attribute of the `PxDatabase` object like so:
`db.language = "en"`
This will change the response language for all subsequent queries to the API.

From here we can also browse around and get data. Checking out all tables available is doable `~~.PxDatabase.all_tables()`, but probably a bit overwhelming.

```{python}
db.all_tables()
```

Tables are organised into categories, or `paths`.
```{python}
db.get_paths()
```

It's also possible to filter the paths, for example to get all paths related to "Befolkning".

```{python}
db.get_paths(path_id="BE")
```

To get all tables that's in a specific path like "Folkmängd", use `~~.PxDatabase.tables_in_path()`.

```{python}
db.tables_in_path(path_id="BE0101A")
```

## Searching for tables
It's also possible to search for tables in the database using the `~~.PxDatabase.search()` method.

```{python}
# Keeping it simple and just look for tables updated in the past 30 days matching the query string
results = db.search(query="energi", past_days=30)

# Checking how many tables there are in the results
len(results.get("tables"))
```
We can also get the labels and ID's, or any other metadata, to find out more.
```{python}
[
    {k: v for k, v in table.items() if k in ("id", "label")}
    for table in results.get("tables")
]
```

## Getting table metadata
There are two methods to get table metadata. You can get the full metadata information by simply calling `~~.PxDatabase.get_table_metadata()`.

If you're interested in the details about variables of a table you can also use `~~.PxDatabase.get_table_variables()`. This method returns information in a more condensed way which may be easier to overview.

```{python}
# Use the table ID
tab_vars = db.get_table_variables("TAB2706")

# Let's check out Region
tab_vars.get("Region")
``` 

As can be seen above `elimination` is `True` for `"Region"`, so the variable can be skipped over. But there's also a few code lists associated with the variable.

### Code lists
Getting information about code lists can be done with `~~.PxDatabase.get_code_list()`. 

```{python}
# Fetching and unpacking 'values' of 'vs_RegionValkrets99'
db.get_code_list("vs_RegionValkrets99").get("values")
```

The codes can then be used in a query for a selection based on the code list. More on that in [Getting table data](#getting-table-data).

## Getting table data
To get data with `~~.PxDatabase.get_table_data()`  we need a few things.

- A table ID
- A selection of value codes from variables
- A code list (optional)

```{python}
# Getting some election results for specific regions, using a code list to match value codes
dataset = db.get_table_data(
    "TAB2706",
    value_codes={
        "ContentsCode": "ME0104B6",
        "Tid": "2022",
        "Region": ["VR2", "VR3"],
        "Partimm": [
            "M",
            "C",
            "FP",
            "KD",
            "MP",
            "S",
            "V",
            "SD",
            "ÖVRIGA",
            "OGILTIGA",
            "VALSKOLKARE",
        ],
    },
    code_list={"Region": "vs_RegionValkrets99"},
)

# A finished dataset looks like this
dataset
```
### Loading into dataframes
The native format of the returned dataset can now easily be loaded into a dataframe.

For instance `polars`:
```{python}
import polars as pl

pl.DataFrame(dataset)
```

But also `pandas` and `pyarrow`:
```{python}
import pandas as pd

pd.DataFrame(dataset)
```

```{python}
import pyarrow as pa

pa.Table.from_pylist(dataset)
```

### Using wildcards
Wildcards are useful, and here is an example using wildcards for a larger query.

```{python}
# Using wildcards here to get all the municipalities in Stockholm, all months of 2024, all genders and 5-year age groups.
# The somewhat cryptic ContentsCode represents count
population_data = db.get_table_data(
    "TAB5444",
    value_codes={
        "Alder": "*",
        "Region": "01*",
        "Tid": "2024*",
        "Kon": "*",
        "ContentsCode": "000003O5",
    },
    code_list={"Alder": "agg_Ålder5år", "Region": "vs_RegionKommun07"},
)

# This returns over ten thousand rows of data
len(population_data)

```

### Large queries and batching
pxwebpy allows for very large queries by using automatic batching to stay within the rate limits of the API. 

Consider the following query for population per year (`"TAB1267"`):

```{python}
codes = {"ContentsCode": "BE0101A9", "Region": "*", "Alder": "*", "Kon": "*", "Tid": "*"}
lists = {"Region": "vs_RegionKommun07", "Alder": "vs_Ålder1årA"}
```

This query would produce over 1 million data cells, overshooting the data cell limit of the API (150 000 in this case). 

To handle this pxwebpy will break up the query into several subqueries to stay within the limit of data cells while also respecting the rate limit of the number of queries allowed within a give time window. Calls are multithreaded to fetch results as fast as possible.

```{python}
# Executing the large query
data = db.get_table_data("TAB1267", value_codes=codes, code_list=lists)

# And then loading the result into a dataframe
pl.DataFrame(data)
```

### In-memory caching
By default pxwebpy uses in-memory caching for API responses, which can be useful for exploration and iterative use. Caching both reduces the load on the API and speeds up execution. However it can be turned off if needed simply by setting the attribute `disable_cache` to `True`.
